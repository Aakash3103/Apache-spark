{"cells":[{"cell_type":"code","source":["#Notes\n'''all the actions are case sensitive please be carefull while writing .'''"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"63abac5e-18b3-4652-b056-b5cff8d4ef07"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class='ansiout'>'all the actions are case sensitive please be carefull while writing .'</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class='ansiout'>'all the actions are case sensitive please be carefull while writing .'</div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["from pyspark import SparkContext"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3457308d-b0c9-4a55-a866-c9086f9b2354"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["sc = SparkContext.getOrCreate()\nprint(sc)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"28658e04-b5ed-45ec-b0f1-405ee6db0d48"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class='ansiout'>&lt;SparkContext master=local[8] appName=Databricks Shell&gt;\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class='ansiout'>&lt;SparkContext master=local[8] appName=Databricks Shell&gt;\n</div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["my_rdd = sc.parallelize([1,2,3,4,5,6,7,8,9,10,11,11,12,13,14,15])"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"428a1aef-d7e8-4e33-bdd8-af854e914af4"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["my_rdd.collect()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1dfad88a-924e-42e1-a7ac-35de61d347dd"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class='ansiout'>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 11, 12, 13, 14, 15]</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class='ansiout'>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 11, 12, 13, 14, 15]</div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["print(my_rdd.take(3))#display first 3 elements\nprint(my_rdd.top(3)) #display last 3 elements\nprint(my_rdd.min())#display minimum\nprint(my_rdd.max())#display maximum\nprint(my_rdd.sum())#doing sum of all\nprint(my_rdd.mean())#mean\nprint(my_rdd.stdev())#standard deviation\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"05d1074d-f873-46b5-8930-5192bbac73b8"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class='ansiout'>[1, 2, 3]\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class='ansiout'>[1, 2, 3]\n</div>"]},"transient":null},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class='ansiout'>[15, 14, 13]\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class='ansiout'>[15, 14, 13]\n</div>"]},"transient":null},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class='ansiout'>1\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class='ansiout'>1\n</div>"]},"transient":null},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class='ansiout'>15\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class='ansiout'>15\n</div>"]},"transient":null},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class='ansiout'>131\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class='ansiout'>131\n</div>"]},"transient":null},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class='ansiout'>8.1875\n4.245861956069698\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class='ansiout'>8.1875\n4.245861956069698\n</div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["#take a subset of list from list/want few samples\nprint(my_rdd.takeSample(True,9)) \nprint(my_rdd.takeSample(False,9)) \n\n#Repeated Values:\n#true = give some repeated values if have\n#false = give unique values"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"46b9bdd8-d6c0-4c00-b756-b9db849e1951"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class='ansiout'>[2, 8, 14, 9, 1, 13, 15, 7, 11]\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class='ansiout'>[2, 8, 14, 9, 1, 13, 15, 7, 11]\n</div>"]},"transient":null},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class='ansiout'>[5, 15, 11, 10, 14, 6, 12, 4, 11]\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class='ansiout'>[5, 15, 11, 10, 14, 6, 12, 4, 11]\n</div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["#--------------------------------------------#"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"34dab2a7-4aef-4481-bac2-6c6faf12074d"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#reduce is a spark action that aggregates a dataset element\nrdd = sc.parallelize([0,1,2,3,4,5,6,7,8,9])\nrdd.reduce(lambda a,b:a+b)\n  "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8f9ce8c9-55cb-412c-9e26-af551ba1f8bf"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[1]: 45</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[1]: 45</div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["#COUNT THE ELEMENT\nx = sc.parallelize([1,1,1,2,1,4,3,4,4,6,3,6,7,8,6,])\nx.count()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bb0c9d54-6f37-4bb9-b46e-82988ba88437"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class='ansiout'>15</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class='ansiout'>15</div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["#COUNT THE VALUE\nx = sc.parallelize([1,1,1,2,1,4,3,4,4,6,3,6,7,8,6,])\nx.countByValue()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c2441611-f72c-44f0-a6e4-2cd09eae9f2d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class='ansiout'>defaultdict(int, {1: 4, 2: 1, 4: 3, 3: 2, 6: 3, 7: 1, 8: 1})</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class='ansiout'>defaultdict(int, {1: 4, 2: 1, 4: 3, 3: 2, 6: 3, 7: 1, 8: 1})</div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["\n#COUNT by keys\nx = sc.parallelize([('A',1),('B',2),('C',5),('C',6)])\n\nx.countByKey()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"19b64c94-4473-4404-b541-1f4910384e2b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class='ansiout'>defaultdict(int, {'A': 1, 'B': 1, 'C': 2})</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class='ansiout'>defaultdict(int, {'A': 1, 'B': 1, 'C': 2})</div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["'''Fold is a very powerful operation in spark which allows you to calculate many important values in O(n) time. \n\ndef fold[T](acc:T)((acc,value) => acc)\nThe above is kind of high level view of fold api. It has following three things\n\n->T is the data type of RDD\n->acc is accumulator of type T which will be return value of the fold operation\n->A function , which will be called for each element in rdd with previous accumulator.'''\n#aggregate the elements of each partition\nfrom operator import add\n\nx_fold = sc.parallelize([1,2,3,4,5])\nD=x_fold.repartition(1)\nD.fold(0,add)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"772279e3-4345-4fbd-890f-e440af3d279a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class='ansiout'>15</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class='ansiout'>15</div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["print(x_fold.getNumPartitions())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3cfd5c65-087e-40b4-a58b-234445720d92"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class='ansiout'>8\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class='ansiout'>8\n</div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["x_fold.glom().collect()\n#go through \" https://www.youtube.com/watch?v=h9ns7DHS6fI \""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"08bae910-2096-42f7-8673-b53888fb35d0"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class='ansiout'>[[], [1], [], [2], [3], [], [4], [5]]</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class='ansiout'>[[], [1], [], [2], [3], [], [4], [5]]</div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["#Range \nx_range = sc.parallelize(range(0,10))#or(range(20))\nx_range.collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"178f19fa-f39a-47f1-b297-41994e022ef2"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class='ansiout'>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class='ansiout'>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]</div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["#variance\nx_variance = sc.parallelize([1,2,3,4,5,6,7])\nx_variance.variance()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a69096ce-6aae-48dc-b950-648b0dd28fb9"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class='ansiout'>4.0</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class='ansiout'>4.0</div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["'''\ndifference between variance and sample variance is variance can take n value but sample variance can take n-1 values as an input\n'''"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a7c5c119-20c3-4323-8548-83f90ff21611"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#samplevariance\nx_samplevar = sc.parallelize([1,2,3,4,5,6])\nx_samplevar.sampleVariance()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dd68314c-7ce4-45d2-b3c9-8aa75c2588b4"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class='ansiout'>3.5</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class='ansiout'>3.5</div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5e7bb28c-6357-4bdf-96fa-964494bdbcdd"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#save as picklefile\nnRdd= sc.textFile(\"/FileStore/tables/data-1.txt\")\nnRdd.collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3bd156fe-eaad-45b2-b874-053a9a1cb226"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["newvar = sc.parallelize([1,2,3,4,5])"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9354a3b2-7450-4544-b357-57f612adc63d"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["newvar.saveAsTextFile(\"DBFS/FileStore/rdddata1/\") "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"82a6307e-4cfe-4b4b-b0d8-a46269506176"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"---------------------------------------------------------------------------\nPy4JJavaError                             Traceback (most recent call last)\n<command-3659691617658234> in <module>\n----> 1 newvar.saveAsTextFile(\"DBFS/FileStore/rdddata1/\")\n\n/databricks/spark/python/pyspark/rdd.py in saveAsTextFile(self, path, compressionCodecClass)\n   1861             self.ctx._jvm.PythonRDD.saveAsTextFileImpl(keyed._jrdd, path, compressionCodecClass)\n   1862         else:\n-> 1863             self.ctx._jvm.PythonRDD.saveAsTextFileImpl(keyed._jrdd, path)\n   1864 \n   1865     # Pair functions\n\n/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py in __call__(self, *args)\n   1302 \n   1303         answer = self.gateway_client.send_command(command)\n-> 1304         return_value = get_return_value(\n   1305             answer, self.gateway_client, self.target_id, self.name)\n   1306 \n\n/databricks/spark/python/pyspark/sql/utils.py in deco(*a, **kw)\n    115     def deco(*a, **kw):\n    116         try:\n--> 117             return f(*a, **kw)\n    118         except py4j.protocol.Py4JJavaError as e:\n    119             converted = convert_exception(e.java_exception)\n\n/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)\n    324             value = OUTPUT_CONVERTER[type](answer[2:], gateway_client)\n    325             if answer[1] == REFERENCE_TYPE:\n--> 326                 raise Py4JJavaError(\n    327                     \"An error occurred while calling {0}{1}{2}.\\n\".\n    328                     format(target_id, \".\", name), value)\n\nPy4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.saveAsTextFileImpl.\n: org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory dbfs:/DBFS/FileStore/rdddata1 already exists\n\tat org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:131)\n\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.assertConf(SparkHadoopWriter.scala:302)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:75)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1090)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:125)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:419)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1088)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1061)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:125)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:419)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1026)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1008)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:125)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:419)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1007)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$2(PairRDDFunctions.scala:964)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:125)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:419)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:962)\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$2(RDD.scala:1601)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:125)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:419)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1601)\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$1(RDD.scala:1587)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:125)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:419)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1587)\n\tat org.apache.spark.api.java.JavaRDDLike.saveAsTextFile(JavaRDDLike.scala:560)\n\tat org.apache.spark.api.java.JavaRDDLike.saveAsTextFile$(JavaRDDLike.scala:559)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:45)\n\tat org.apache.spark.api.python.PythonRDD$._saveAsTextFile(PythonRDD.scala:891)\n\tat org.apache.spark.api.python.PythonRDD$.saveAsTextFileImpl(PythonRDD.scala:859)\n\tat org.apache.spark.api.python.PythonRDD.saveAsTextFileImpl(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\n","errorSummary":"org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory dbfs:/DBFS/FileStore/rdddata1 already exists","metadata":{},"errorTraceType":"html","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n---------------------------------------------------------------------------\nPy4JJavaError                             Traceback (most recent call last)\n<command-3659691617658234> in <module>\n----> 1 newvar.saveAsTextFile(\"DBFS/FileStore/rdddata1/\")\n\n/databricks/spark/python/pyspark/rdd.py in saveAsTextFile(self, path, compressionCodecClass)\n   1861             self.ctx._jvm.PythonRDD.saveAsTextFileImpl(keyed._jrdd, path, compressionCodecClass)\n   1862         else:\n-> 1863             self.ctx._jvm.PythonRDD.saveAsTextFileImpl(keyed._jrdd, path)\n   1864 \n   1865     # Pair functions\n\n/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py in __call__(self, *args)\n   1302 \n   1303         answer = self.gateway_client.send_command(command)\n-> 1304         return_value = get_return_value(\n   1305             answer, self.gateway_client, self.target_id, self.name)\n   1306 \n\n/databricks/spark/python/pyspark/sql/utils.py in deco(*a, **kw)\n    115     def deco(*a, **kw):\n    116         try:\n--> 117             return f(*a, **kw)\n    118         except py4j.protocol.Py4JJavaError as e:\n    119             converted = convert_exception(e.java_exception)\n\n/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)\n    324             value = OUTPUT_CONVERTER[type](answer[2:], gateway_client)\n    325             if answer[1] == REFERENCE_TYPE:\n--> 326                 raise Py4JJavaError(\n    327                     \"An error occurred while calling {0}{1}{2}.\\n\".\n    328                     format(target_id, \".\", name), value)\n\nPy4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.saveAsTextFileImpl.\n: org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory dbfs:/DBFS/FileStore/rdddata1 already exists\n\tat org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:131)\n\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.assertConf(SparkHadoopWriter.scala:302)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:75)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1090)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:125)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:419)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1088)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1061)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:125)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:419)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1026)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1008)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:125)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:419)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1007)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$2(PairRDDFunctions.scala:964)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:125)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:419)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:962)\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$2(RDD.scala:1601)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:125)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:419)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1601)\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$1(RDD.scala:1587)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:125)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:419)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1587)\n\tat org.apache.spark.api.java.JavaRDDLike.saveAsTextFile(JavaRDDLike.scala:560)\n\tat org.apache.spark.api.java.JavaRDDLike.saveAsTextFile$(JavaRDDLike.scala:559)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:45)\n\tat org.apache.spark.api.python.PythonRDD$._saveAsTextFile(PythonRDD.scala:891)\n\tat org.apache.spark.api.python.PythonRDD$.saveAsTextFileImpl(PythonRDD.scala:859)\n\tat org.apache.spark.api.python.PythonRDD.saveAsTextFileImpl(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["newvar.collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"99a2e2cb-a737-4d9a-9862-804e73572d3d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class='ansiout'>[1, 2, 3, 4, 5]</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class='ansiout'>[1, 2, 3, 4, 5]</div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["\nrdd_emp.coalesce(1).saveAsTextFile(\"/FileStore/tables/data-1.txt\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d994415f-966d-42ef-80c2-939db172cb2d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\n<command-3659691617658243> in <module>\n----> 1 rdd_emp.coalesce(1).saveAsTextFile(\"/FileStore/tables/data-1.txt\")\n\nNameError: name 'rdd_emp' is not defined","errorSummary":"<span class='ansi-red-fg'>NameError</span>: name 'rdd_emp' is not defined","metadata":{},"errorTraceType":"html","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\n<command-3659691617658243> in <module>\n----> 1 rdd_emp.coalesce(1).saveAsTextFile(\"/FileStore/tables/data-1.txt\")\n\nNameError: name 'rdd_emp' is not defined"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["#transformation \n'''\nspark transformation is a function that produces new RDD from the existing RDDs. It takes RDD as input and produces one or mote RDD as output. Each time it creates new RDD when we apply any transformation.\n\nOR:->lazy execution is there:delay execution untill finds an action so that it can prepare optimized lineage....\n\ngo through:  \"  https://drive.google.com/drive/folders/1jR8bg4V2_24kquRpthsFTmDuWQmK1UwJ \"\n'''"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"44b53169-ed99-4fa5-bab5-563b016d1fb4"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class='ansiout'>'\\nspark transformation is a function that produces new RDD from the existing RDDs. It takes RDD as input and produces one or mote RDD as output. Each time it creates new RDD when we apply any transformation.\\n\\nOR:-&gt;lazy execution is there:delay execution untill finds an action so that it can prepare optimized lineage....\\n\\ngo through:  &quot;  https://drive.google.com/drive/folders/1jR8bg4V2_24kquRpthsFTmDuWQmK1UwJ &quot;\\n'</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class='ansiout'>'\\nspark transformation is a function that produces new RDD from the existing RDDs. It takes RDD as input and produces one or mote RDD as output. Each time it creates new RDD when we apply any transformation.\\n\\nOR:-&gt;lazy execution is there:delay execution untill finds an action so that it can prepare optimized lineage....\\n\\ngo through:  &quot;  https://drive.google.com/drive/folders/1jR8bg4V2_24kquRpthsFTmDuWQmK1UwJ &quot;\\n'</div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["#narrow:map():map funtion iterates over every line in RDD and split into new RDD\n\nx_map = sc.parallelize([1,2,3,4,5,6])\ny_map = x_map.map(lambda x:(x+2))#eg-(x,x+2)....etc\ny_map.collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"17f23ad4-6fb6-4183-b0ee-e58b19761598"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[15]: [3, 4, 5, 6, 7, 8]</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[15]: [3, 4, 5, 6, 7, 8]</div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["#narrow:filter():returns a new RDD, containing only the elements that meet a predicate.\n#narrow operation because it does not shuffle data from one partition to many partition\n\nx_filter = sc.parallelize([1,2,3,4,5,6,7,8,9])\ny_filter = x_filter.filter(lambda x:x%2==0)\ny_filter.collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1f0f4a28-0dc4-4f8e-a9a9-220a3ca59e4b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class='ansiout'>[2, 4, 6, 8]</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class='ansiout'>[2, 4, 6, 8]</div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["#narrow:flatMap():in this each input have many elements in an output RDD.or Split each input string into words.\n#multiple or secuence value as we do not do this in map() because this is for single output/input\n\n\nx_flatMap = sc.parallelize([1,2,3,4,5,6,7,8,9])\ny_flatMap = x_flatMap.flatMap(lambda x:(x,x**2))#like use for multiple works\ny_flatMap.collect()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3eb6a2d6-3777-4916-9cd6-dc9ecf5942e8"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class='ansiout'>[1, 1, 2, 4, 3, 9, 4, 16, 5, 25, 6, 36, 7, 49, 8, 64, 9, 81]</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class='ansiout'>[1, 1, 2, 4, 3, 9, 4, 16, 5, 25, 6, 36, 7, 49, 8, 64, 9, 81]</div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["#mapPartitions():similar to map,but runs separately on each partition of the RDD\n\"\"\"\n converts each partition of the source RDD into many elements of the result (possibly none). In mapPartition(), the map() function is applied on each partitions simultaneously. MapPartition is like a map, but the difference is it runs separately on each partition(block) of the RDD.\n\"\"\"\nx = sc.parallelize([1,2,3,4,5,6,7,8,9,],3 )\ndef f(iter):\n  yield sum(iter)\ny=x.mapPartitions(f)\ny.collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b151b38f-a5d1-4a5d-8f1b-7ed1576b7838"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class='ansiout'>[6, 15, 24]</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class='ansiout'>[6, 15, 24]</div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["#glom():flattens elements on the same partitions\nprint(x.glom().collect())\n\nprint(y.glom().collect())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a67ba089-49ae-40e6-8adb-7301feca0fce"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class='ansiout'>[[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n[[6], [15], [24]]\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class='ansiout'>[[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n[[6], [15], [24]]\n</div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["#mapPartitionsWithIndex():\n'''\n mapPartition it provides func with an integer value representing the index of the partition, and the map() is applied on partition index wise one after the other.\n \n '''\n#here we can see the partitions including the sum.....\nx = sc.parallelize([1,2,3,4,5,6,7,8,9,],2 )\ndef f(pos,iter):\n  yield (pos,sum(iter))\ny=x.mapPartitionsWithIndex(f)\ny.collect()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c837dd18-4a8d-4da3-bb60-53b412aa25f1"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class='ansiout'>[(0, 10), (1, 35)]</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class='ansiout'>[(0, 10), (1, 35)]</div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["#sample():Sample a fraction fraction of the data, with or without replacement, using a given random number generator seed.\n#sample(withReplacement:True/False,fraction,seed=None/optional)\nx = sc.parallelize([1,2,3,4,5,6,7,8,9])\ny = x.sample(True,0.3)\ny.collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4500f914-e281-4c33-9154-499905ae222a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class='ansiout'>[6]</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class='ansiout'>[6]</div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["#union\nx = sc.parallelize([3,2,4,7])\ny = sc.parallelize([3,4,6,9,0])\nx.union(y).collect()\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"76c0bc88-7326-439b-9197-d9ac2031509e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class='ansiout'>[3, 2, 4, 7, 3, 4, 6, 9, 0]</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class='ansiout'>[3, 2, 4, 7, 3, 4, 6, 9, 0]</div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["#intersection\nx = sc.parallelize([3,2,4,7])\ny = sc.parallelize([3,4,6,9,0])\nx.intersection(y).collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"96e7a885-6bd0-4172-aa25-d3084baa1017"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class='ansiout'>[3, 4]</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class='ansiout'>[3, 4]</div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["#distinct:duplicates is removed\nx = sc.parallelize([3,3,3,2,4,7])\n\nx.distinct().collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c53c50cd-ac15-4134-a8b7-94f309ef30eb"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class='ansiout'>[2, 3, 4, 7]</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class='ansiout'>[2, 3, 4, 7]</div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["#groupByKey()\nx = sc.parallelize([('A',3),('A',6),('B',2),('C',9)])\ny=x.groupByKey()\ny.collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e17e01c8-f0fe-48d1-9fe9-850ab6de0267"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#Please don't use this because because this is closure problem we know that we initiate the spark context or the driver code and initialise our counter as zero and after that create an RDD out of that and execute our program.What happens is like drivers submit the program to the worker node because we have three or may be more worker node and they having individual executor in each worker node.driver initialize a serialized copy of counter =0 to each of working node to use it for a particular task. But actually all the executors play with the counter not do anything with it. Thats why they return counter as 0.\n\n#like in laymen language Datanode holds the RDD part and the increment of counter is made by driver program so when the counter is initialize to 0 they save it to 0.\n#To solve this closure problem, Spark introduce a concept called accumulator, it will aggregate values from worker back to driver. Only driver can access the value of a accumulator. For task, accumulator are write only.\ncounter = 0\n\nrdd = sc.parallelize([1,2,3,4,5,6,7,8])\n\n# Wrong: Don't do this!!\ndef increment_counter(x):\n    global counter\n    counter += x\nrdd.foreach(increment_counter)\n\nprint(\"Counter value: \", counter)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"59acf5d5-3334-45af-9a83-ddc24f7a5e8c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Counter value:  0\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Counter value:  0\n</div>"]},"transient":null}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"RDD_actions_transformations","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":3659691617658211}},"nbformat":4,"nbformat_minor":0}
